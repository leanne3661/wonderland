---
title: "Lewis Carroll and Alice's Adventure"
author: "Leanne Fortney"
date: "April 11, 2017"
output: html_document
---

```{r, include=FALSE}
library(tidyverse)
library(stringr)
library(tokenizers)
library(tidytext)
library(topicmodels)
library(dplyr)
library(gutenbergr)
```
```{r, message=FALSE, warning=FALSE, include=FALSE}
gutenberg_works(str_detect(author, "Carroll"))
books <- gutenberg_download(c(11, 12, 13, 620, 651, 19002, 28696, 28885, 29042, 29888, 33582, 35497, 35535, 36308, 38308, 38065, 48630, 48795), meta_fields = "title")

books <- books %>% 
mutate(words = count_words(text))
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}
books

sum(books$words)
ggplot(books, aes(x = words)) + 
geom_histogram(binwidth = 1) +
  labs(title = "Lengths of narratives")
```
```{r, include=FALSE}
words <- books %>%
  unnest_tokens(word, text)

books_tokenized <- books %>% 
  select(title, text) %>% 
  unnest_tokens(word, text, token = "words")

word_counts <- books_tokenized %>% 
  count(word, sort=TRUE)

before <- nrow(books_tokenized)
  
words_to_drop <- word_counts %>% 
filter(n <= 2 | n >= 8000)

nrow(words_to_drop) / nrow(word_counts)

books_tokenized <- books_tokenized %>% 
  anti_join(words_to_drop, by = "word") %>% 
  anti_join(stop_words, by = "word")

after <- nrow(books_tokenized)
before - after
after / before

```
```{r, echo=FALSE}
plot_words <- function(tidy_df, n = 10) {
  require(ggplot2)
  require(dplyr)
  tidy_df %>%
    count(word, sort = TRUE) %>%
    top_n(n = n, n) %>% 
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
}
plot_words(books_tokenized, n = 60)

```

```{r, message=FALSE, warning=FALSE, include=FALSE}
alice <- gutenberg_download((11), meta_fields = "title")

alice <- alice %>% 
mutate(words = count_words(text))
```
```{r, echo=FALSE}
alice

sum(alice$words)
ggplot(alice, aes(x = words)) + 
geom_histogram(binwidth = 1) +
  labs(title = "Lengths of narratives")
```
```{r, include=FALSE}
words <- alice %>%
  unnest_tokens(word, text)

alice_tokenized <- alice %>% 
  select(title, text) %>% 
  unnest_tokens(word, text, token = "words")

  word_counts <- alice_tokenized %>% 
  count(word, sort = TRUE)

  before <- nrow(alice_tokenized)
  
  words_to_drop <- word_counts %>% 
  filter(n <= 2 | n >= 8000)

nrow(words_to_drop) / nrow(word_counts)

alice_tokenized <- alice_tokenized %>% 
  anti_join(words_to_drop, by = "word") %>% 
  anti_join(stop_words, by = "word")

after <- nrow(alice_tokenized)
before - after
after / before
```
```{r, echo=FALSE}
plot_words <- function(tidy_df, n = 10) {
  require(ggplot2)
  require(dplyr)
  tidy_df %>%
    count(word, sort = TRUE) %>%
    top_n(n = n, n) %>% 
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
}
plot_words(alice_tokenized, n = 60)

```
