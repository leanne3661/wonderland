---
title: "Lewis Carroll and Alice's Adventure"
author: "Leanne Fortney"
date: "April 11, 2017"
output: html_document
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(stringr)
library(tokenizers)
library(tidytext)
library(topicmodels)
library(dplyr)
library(gutenbergr)
library(textreuse)
```
Lewis Carroll is famous for her books about Alice and her adventures in Wonderland. Taken from the Gutenberg Projects, the following two visualizations display the word count per line within a given text. The second displays the top ten commonly used words, include the use of the numbers 2, 3, 4. 

```{r, include=FALSE}
gutenberg_works(str_detect(author, "Carroll"))
books <- gutenberg_download(c(11, 12, 13, 620, 651, 19002, 28696, 28885, 29042, 29888, 33582, 35497, 35535, 36308, 38308, 38065, 48630, 48795), meta_fields = "title")

books <- books %>% 
mutate(words = count_words(text))

books
sum(books$words)
```
```{r, echo=FALSE}
ggplot(books, aes(x = words)) +
geom_histogram(binwidth = 1, 
                  col= "black",
               aes(fill=..count..)) +
scale_fill_gradient("Count", low = "blue", high = "red") +
  coord_cartesian(xlim = c(2, 20), ylim = c(2, 10000)) +
  labs(title = "Lengths of narratives")
```
```{r, message=TRUE, warning=TRUE, include=FALSE}
words <- books %>%
  unnest_tokens(word, text)

books_tokenized <- books %>% 
  select(title, text) %>% 
  unnest_tokens(word, text, token = "words")

word_counts <- books_tokenized %>% 
  count(word, sort=TRUE)

before <- nrow(books_tokenized)
  
words_to_drop <- word_counts %>% 
filter(n <= 2 | n >= 1500)

nrow(words_to_drop) / nrow(word_counts)

books_tokenized <- books_tokenized %>% 
  anti_join(words_to_drop, by = "word") %>% 
  anti_join(stop_words, by = "word")

after <- nrow(books_tokenized)
before - after
after / before

```
Visualization 2: The Top Ten Words

```{r, echo=FALSE, message=TRUE, warning=TRUE}
plot_words <- function(tidy_df, n = 20) {
  require(ggplot2)
  require(dplyr)
  tidy_df %>%
    count(word, sort = TRUE) %>%
    top_n(n = n, n) %>% 
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
}
```
```{r, echo=FALSE}
plot_words(books_tokenized, n = 20)
```


```{r, include=FALSE}
alice <- gutenberg_download((11), meta_fields = "title")

alice <- alice %>% 
mutate(words = count_words(text))
sum(alice$words)
```
These are only text within Alice's Adventures in Wonderland.
```{r, echo=FALSE}
ggplot(alice, aes(x = words)) + 
geom_histogram(binwidth = 1, 
                  col= "black",
               aes(fill=..count..)) +
scale_fill_gradient("count", low = "yellow", high = "red") +
  coord_cartesian(xlim = c(2, 20), ylim = c(2, 500)) +
  labs(title = "Lengths of narratives")
```

```{r, include=FALSE}
words <- alice %>%
  unnest_tokens(word, text)

alice_tokenized <- alice %>% 
  select(title, text) %>% 
  unnest_tokens(word, text, token = "words")

  word_counts <- alice_tokenized %>% 
  count(word, sort = TRUE)

  before <- nrow(alice_tokenized)
  
  words_to_drop <- word_counts %>% 
  filter(n <= 2 | n >= 1500)

nrow(words_to_drop) / nrow(word_counts)

alice_tokenized <- alice_tokenized %>% 
  anti_join(words_to_drop, by = "word") %>% 
  anti_join(stop_words, by = "word")

after <- nrow(alice_tokenized)
before - after
after / before
```
```{r, echo=FALSE}
plot_words <- function(tidy_df, n = 10) {
  require(ggplot2)
  require(dplyr)
  tidy_df %>%
    count(word, sort = TRUE) %>%
    top_n(n = n, n) %>% 
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
}
```
```{r, echo=FALSE}
plot_words(alice_tokenized, n = 10)
```
```{r, include=FALSE}

minhash <- minhash_generator(n = 5, seed = 15)
lsh_threshold(25, 5)


corpus <- TextReuseCorpus(dir = "Carroll",
                          tokenizer = tokenizers::tokenize_ngrams,
                          n = 5, simplify = TRUE,
                          minhash_func = minhash, keep_tokens = TRUE)

doc_alice <- corpus[["11"]]
doc_looking_glass <- corpus[["12"]]
doc_alice_underground <- corpus[["19002"]]

content(doc_alice)
content(doc_looking_glass)

jaccard_similarity(doc_alice, doc_looking_glass)
jaccard_similarity(doc_alice, doc_alice_underground)

cf_sample <- pairwise_compare(sample(corpus, 3), jaccard_similarity)
cf_sample %>% round(2)

buckets <- lsh(corpus, bands = 5)
```
```{r}
align_local(doc_alice, doc_looking_glass)
align_local(corpus[["11"]], corpus[["19002"]])
```


